{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder v2\n",
    "Re-implemented v1 with enhancements\n",
    "\n",
    "### Information\n",
    "1. Eight-layer architecture (4 in encoder + 4 in decoder)\n",
    "2. Dataset - 1000 STL files\n",
    "3. Train - 800 models\n",
    "4. Test - 200 models\n",
    "5. Epochs - 50\n",
    "6. Loss - 0.0214\n",
    "\n",
    "### Encoder architecture\n",
    "- Conv3D(32, (3, 3, 3), activation='elu', padding='same')\n",
    "- MaxPooling3D((2, 2, 2), padding='same')\n",
    "- Conv3D(16, (3, 3, 3), activation='elu', padding='same')\n",
    "- MaxPooling3D((2, 2, 2), padding='same')\n",
    "\n",
    "### Changelog\n",
    "#### First attempt\n",
    "- Modified ConversionUtils.py so that the point clouds generated have 15k points instead of 10k\n",
    "- Result: Loss reduced from 0.0266 to 0.0230\n",
    "#### Second attempt\n",
    "- Switched from ReLU to ELU\n",
    "- Result: Loss reduced from 0.0230 to 0.0216\n",
    "#### Third attempt\n",
    "- Modified ConversionUtils.py so that the point clouds generated have 20k points instead of 15k\n",
    "- Result: Loss reduced from 0.0216 to 0.0214"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing all necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.layers import Input, Conv3D, MaxPooling3D, UpSampling3D, GlobalMaxPooling3D\n",
    "from keras.models import Model\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyntcloud import PyntCloud\n",
    "import open3d as o3d\n",
    "from ConversionUtils import ConversionUtils\n",
    "from Visualization import Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting STL to point cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Execute the following cell if you wish to delete all existing point cloud files in abc-dataset-ply/ directory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"abc-dataset-ply/\"\n",
    "files = os.listdir(folder_path)\n",
    "\n",
    "for file in files:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    if os.path.isfile(file_path):\n",
    "        try:\n",
    "            os.remove(file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error deleting {file}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Only execute the next cell if the point cloud files do not exist in abc-dataset-ply/ directory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = sorted(ConversionUtils.list_files_in_directory(\"abc-dataset-stl/\"))\n",
    "for i in files:\n",
    "    path = \"abc-dataset-stl/\" + i\n",
    "    ConversionUtils.stl_to_ply(path, 20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting point cloud to binary voxel arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"abc-dataset-ply/\"\n",
    "# Taking first 1000 models\n",
    "files = sorted([filename for filename in os.listdir(directory) if os.path.isfile(os.path.join(directory, filename))])\n",
    "dataset = []\n",
    "\n",
    "for i in files:\n",
    "    path = os.path.join(directory, i)\n",
    "    binvox = ConversionUtils.convert_to_binvox(path, 64)\n",
    "    dataset.append(binvox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting dataset for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset into numpy arrays\n",
    "dataset = np.array(dataset)\n",
    "\n",
    "# Split your dataset into train and test datasets\n",
    "train_dataset = dataset[:800]  # Adjust the number as needed\n",
    "test_dataset = dataset[800:]   # The remaining data for testing\n",
    "print(len(train_dataset), len(test_dataset))\n",
    "# Define the input shape\n",
    "input_shape = (64, 64, 64, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = Input(shape=input_shape)\n",
    "x = Conv3D(32, (3, 3, 3), activation='elu', padding='same')(input_data)\n",
    "x = MaxPooling3D((2, 2, 2), padding='same')(x)\n",
    "x = Conv3D(16, (3, 3, 3), activation='elu', padding='same')(x)\n",
    "encoded = MaxPooling3D((2, 2, 2), padding='same')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Conv3D(16, (3, 3, 3), activation='elu', padding='same')(encoded)\n",
    "x = UpSampling3D((2, 2, 2))(x)\n",
    "x = Conv3D(32, (3, 3, 3), activation='elu', padding='same')(x)\n",
    "x = UpSampling3D((2, 2, 2))(x)\n",
    "decoded = Conv3D(1, (3, 3, 3), activation='sigmoid', padding='same')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training autoencoder, prediction done on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = Model(input_data, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "autoencoder.fit(train_dataset, train_dataset, epochs=50, batch_size=10, validation_data=(test_dataset, test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_data = autoencoder.predict(test_dataset, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Model(inputs=input_data, outputs=encoded)\n",
    "encoded_data = encoder.predict(test_dataset, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of input data: \", test_dataset[0].shape)\n",
    "print(\"Shape of encoded data: \", encoded_data[0].shape)\n",
    "print(\"Shape of reconstructed data: \", reconstructed_data[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will convert the encoded data of each model (which is of shape 16x16x16x16) to a 1D array of 65536 elements\n",
    "encoded_data_flattened = encoded_data[0].flatten()\n",
    "print(\"Shape of encoded data after flattening: \", encoded_data_flattened.shape)\n",
    "# This will convert the above array of 65536 elements back to the original encoder output of 8x8x8x8 dimensions (4D array)\n",
    "encoded_regenerated = encoded_data_flattened.reshape(16, 16, 16, 16)\n",
    "print(\"Shape of encoded data after reshaping flattened array format: \", encoded_regenerated.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample reconstruction from test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "index = 0\n",
    "\n",
    "original_sample = test_dataset[index]\n",
    "\n",
    "reconstructed_sample = reconstructed_data[index].reshape(64, 64, 64)\n",
    "threshold = 0.35\n",
    "reconstructed_sample = (reconstructed_sample > threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Visualization.matplotlib_visualize_original(original_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Visualization.matplotlib_visualize_reconstructed(reconstructed_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Visualization.open3d_visualize_original(original_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Visualization.open3d_visualize_reconstructed(reconstructed_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"sample-outputs/v2/\" + \"original-\" + str(index) + \".ply\"\n",
    "o3d.io.write_point_cloud(path, original_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"sample-outputs/v2/\" + \"reconstructed-\" + str(index) + \".ply\"\n",
    "o3d.io.write_point_cloud(path, reconstructed_sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
