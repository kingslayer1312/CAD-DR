{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder v2\n",
    "## Details:\n",
    "1. 6 layers each in encoder and decoder (3 Conv3d, 3 MaxPooling3D)\n",
    "2. Dataset - 1000 STL files\n",
    "3. Train - 800 models\n",
    "4. Test - 200 models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing all necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.layers import Input, Conv3D, MaxPooling3D, UpSampling3D, Flatten, Reshape\n",
    "from keras.models import Model\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyntcloud import PyntCloud\n",
    "import open3d as o3d\n",
    "from ConversionUtils import ConversionUtils\n",
    "from Visualization import Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting STL to point cloud\n",
    "**Only execute the next cell if the point cloud files do not exist in abc-dataset-ply/ directory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = sorted(ConversionUtils.list_files_in_directory(\"abc-dataset-stl/\"))\n",
    "for i in files:\n",
    "    path = \"abc-dataset-stl/\" + i\n",
    "    ConversionUtils.stl_to_ply(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting point cloud to binary voxel arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"abc-dataset-ply/\"\n",
    "files = sorted([filename for filename in os.listdir(directory) if os.path.isfile(os.path.join(directory, filename))])[:1000]\n",
    "dataset = []\n",
    "\n",
    "for i in files:\n",
    "    path = os.path.join(directory, i)\n",
    "    binvox = ConversionUtils.convert_to_binvox(path)\n",
    "    dataset.append(binvox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting dataset for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset into numpy arrays\n",
    "# Here we are using 1000 models in train dataset, and 200 in test dataset\n",
    "# Split your dataset into train and test datasets\n",
    "train_dataset = dataset[:800]  # Adjust the number as needed\n",
    "test_dataset = dataset[800:]   # The remaining data for testing\n",
    "# Define the input shape\n",
    "input_shape = (64, 64, 64, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = Input(shape=input_shape)\n",
    "x = Conv3D(32, (3, 3, 3), activation='relu', padding='same')(input_data)\n",
    "x = MaxPooling3D((2, 2, 2), padding='same')(x)\n",
    "x = Conv3D(16, (3, 3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling3D((2, 2, 2), padding='same')(x)\n",
    "x = Conv3D(8, (3, 3, 3), activation='relu', padding='same')(x)\n",
    "encoded = MaxPooling3D((2, 2, 2), padding='same')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Conv3D(8, (3, 3, 3), activation='relu', padding='same')(encoded)\n",
    "x = UpSampling3D((2, 2, 2))(x)\n",
    "x = Conv3D(16, (3, 3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling3D((2, 2, 2))(x)\n",
    "x = Conv3D(32, (3, 3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling3D((2, 2, 2))(x)\n",
    "decoded = Conv3D(1, (3, 3, 3), activation='sigmoid', padding='same')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training autoencoder, prediction done on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = Model(input_data, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "autoencoder.fit(train_dataset, train_dataset, epochs=50, batch_size=10, validation_data=(test_dataset, test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_data = autoencoder.predict(test_dataset, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Model(inputs=input_data, outputs=encoded)\n",
    "encoded_data = encoder.predict(test_dataset, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of input data: \", test_dataset[0].shape)\n",
    "print(\"Shape of encoded data: \", encoded_data[0].shape)\n",
    "print(\"Shape of reconstructed data: \", reconstructed_data[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will convert the encoded data of each model (which is of shape 8x8x8x8) to a 1D array of 4096 elements\n",
    "encoded_data_flattened = encoded_data[0].flatten()\n",
    "print(\"Shape of encoded data after flattening: \", encoded_data_flattened.shape)\n",
    "# This will convert the above array of 4096 elements back to the original encoder output of 8x8x8x8 dimensions (4D array)\n",
    "encoded_regenerated = encoded_data_flattened.reshape(8, 8, 8, 8)\n",
    "print(\"Shape of encoded data after reshaping flattened array format: \", encoded_regenerated.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample reconstruction from test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "index = 35\n",
    "\n",
    "original_sample = test_dataset[index]\n",
    "\n",
    "reconstructed_sample = reconstructed_data[index].reshape(64, 64, 64)\n",
    "threshold = 0.35\n",
    "reconstructed_sample = (reconstructed_sample > threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Visualization.matplotlib_visualize_original(original_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Visualization.matplotlib_visualize_reconstructed(reconstructed_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Visualization.open3d_visualize_original(original_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Visualization.open3d_visualize_reconstructed(reconstructed_sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
